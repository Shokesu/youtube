{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Comments Sentiment Analysis \n",
    "Spring 2018\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Basic Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import pandas as pd; import os\n",
    "import csv; import numpy as np\n",
    "import re; import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/andiedonovan/myProjects/Youtube_Python_Project/AndiesBranch/') # change directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "okgo = pd.read_csv('data/OKGOcomments.csv', delimiter=\";\", skiprows=2, encoding='latin-1', engine='python') # read in the data\n",
    "blogs = pd.read_csv('data/Kagel_social_media_blogs.csv', delimiter=\"@@@\", skiprows=2, encoding='latin-1', engine='python') # read in the data\n",
    "tweets = pd.read_csv('data/full-corpus.csv', delimiter=\",\", skiprows=2, encoding='latin-1', engine='python') # read in the data\n",
    "trump = pd.read_csv('data/trump.csv', delimiter=\"@@@\", skiprows=2, encoding='utf-8', error_bad_lines=False, engine='python') \n",
    "df = pd.read_csv('data/data.csv', delimiter=\"@@@\", skiprows=2, encoding='utf-8', engine='python') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Clean Data Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(['Topic', 'TweetId', \"TweetDate\"], axis = 1).dropna()\n",
    "tweets.columns = [\"label\", \"comment\"]\n",
    "tweets.label = tweets.label.replace({'positive': '1.0', 'negative':'-1.0', 'neutral': '0.0', 'irrelevant': '0.0'}, regex=True)\n",
    "tweets['label'] = pd.to_numeric(tweets['label'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs.columns = [\"label\", \"comment\"]\n",
    "blogs['label'] = pd.to_numeric(blogs['label'], errors='coerce')\n",
    "okgo.columns = [\n",
    "  'label','comment','a','b']\n",
    "okgo = okgo.drop(['a', 'b'], axis = 1).dropna() # drop columns 3 and 4 and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([okgo, blogs, tweets], ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"comment\", \"label\"]\n",
    "trump.columns = [\"label\", \"comment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3211</th>\n",
       "      <td>0.0</td>\n",
       "      <td>OmniTouch from #Microsoft makes every surface ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>0.0</td>\n",
       "      <td>RT @dalmaer: WebGL infinite bookcase UI http:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Mission Impossible 3 was excellent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                            comment\n",
       "3211    0.0  OmniTouch from #Microsoft makes every surface ...\n",
       "1815    0.0  RT @dalmaer: WebGL infinite bookcase UI http:/...\n",
       "1270    1.0                 Mission Impossible 3 was excellent"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Remove Non-Alphabetic Characters (including numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"comment\"]= df[\"comment\"].astype(str) \n",
    "trump[\"comment\"]= trump[\"comment\"].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanerFn(b):\n",
    "    for row in range(len(b)):\n",
    "        line = b.loc[row, \"comment\"]\n",
    "        b.loc[row,\"comment\"] = re.sub(\"[^a-zA-Z]\", \" \", line)\n",
    "        \n",
    "def cleanerFn2(b):\n",
    "    for row in range(len(b)):\n",
    "        line = b.iloc[row, 1]\n",
    "        b.iloc[row,1] = re.sub(\"[^a-zA-Z]\", \" \", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanerFn(df)\n",
    "cleanerFn2(data)\n",
    "cleanerFn2(trump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andiedonovan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "sw = stopwords.words('english')\n",
    "nltk.download('stopwords')\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['com_token']=df['comment'].str.lower().str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Remove Stop Words, Lemmatization, Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/andiedonovan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlpFunction(DF):\n",
    "    DF['com_token'] = DF['comment'].str.lower().str.split()\n",
    "    DF['com_remv'] = DF['com_token'].apply(lambda x: [y for y in x if y not in sw])\n",
    "    DF[\"com_lemma\"] = DF['com_remv'].apply(lambda x : [lemmatizer.lemmatize(y) for y in x]) # lemmatization\n",
    "    DF['com_stem'] = DF['com_lemma'].apply(lambda x : [ps.stem(y) for y in x]) # stemming\n",
    "    DF[\"com_full\"] = DF[\"com_stem\"].apply(' '.join)\n",
    "    DF[\"com_tagged\"] = DF['com_token'].apply(lambda x : [nltk.pos_tag(y) for y in x]) #word tagging\n",
    "    DF[\"com_stem_str\"] = DF[\"com_stem\"].apply(', '.join)\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>com_token</th>\n",
       "      <th>com_remv</th>\n",
       "      <th>com_lemma</th>\n",
       "      <th>com_stem</th>\n",
       "      <th>com_full</th>\n",
       "      <th>com_tagged</th>\n",
       "      <th>com_stem_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can he do this  Japanese are trying to be ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[how, can, he, do, this, japanese, are, trying...</td>\n",
       "      <td>[japanese, trying, respectful, lo, gan, logan,...</td>\n",
       "      <td>[japanese, trying, respectful, lo, gan, logan,...</td>\n",
       "      <td>[japanes, tri, respect, lo, gan, logan, care, ...</td>\n",
       "      <td>japanes tri respect lo gan logan care wtf</td>\n",
       "      <td>[[(h, NN), (o, NN), (w, NN)], [(c, VB), (a, DT...</td>\n",
       "      <td>japanes, tri, respect, lo, gan, logan, care, wtf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prick</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[prick]</td>\n",
       "      <td>[prick]</td>\n",
       "      <td>[prick]</td>\n",
       "      <td>[prick]</td>\n",
       "      <td>prick</td>\n",
       "      <td>[[(p, NN), (r, NN), (i, NN), (c, VBP), (k, NN)]]</td>\n",
       "      <td>prick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I think all the weeds are crying</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, think, all, the, weeds, are, crying]</td>\n",
       "      <td>[think, weeds, crying]</td>\n",
       "      <td>[think, weed, cry]</td>\n",
       "      <td>[think, weed, cri]</td>\n",
       "      <td>think weed cri</td>\n",
       "      <td>[[(i, NN)], [(t, NN), (h, NN), (i, NN), (n, VB...</td>\n",
       "      <td>think, weed, cri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lmao Americans in the comment section are acti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[lmao, americans, in, the, comment, section, a...</td>\n",
       "      <td>[lmao, americans, comment, section, acting, li...</td>\n",
       "      <td>[lmao, american, comment, section, acting, lik...</td>\n",
       "      <td>[lmao, american, comment, section, act, like, ...</td>\n",
       "      <td>lmao american comment section act like nuke ja...</td>\n",
       "      <td>[[(l, NN), (m, VBZ), (a, DT), (o, NN)], [(a, D...</td>\n",
       "      <td>lmao, american, comment, section, act, like, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many people want to kill him now</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[how, many, people, want, to, kill, him, now]</td>\n",
       "      <td>[many, people, want, kill]</td>\n",
       "      <td>[many, people, want, kill]</td>\n",
       "      <td>[mani, peopl, want, kill]</td>\n",
       "      <td>mani peopl want kill</td>\n",
       "      <td>[[(h, NN), (o, NN), (w, NN)], [(m, VB), (a, DT...</td>\n",
       "      <td>mani, peopl, want, kill</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  label  \\\n",
       "0  How can he do this  Japanese are trying to be ...    NaN   \n",
       "1                                             Prick     NaN   \n",
       "2                  I think all the weeds are crying     NaN   \n",
       "3  Lmao Americans in the comment section are acti...    NaN   \n",
       "4             How many people want to kill him now      NaN   \n",
       "\n",
       "                                           com_token  \\\n",
       "0  [how, can, he, do, this, japanese, are, trying...   \n",
       "1                                            [prick]   \n",
       "2           [i, think, all, the, weeds, are, crying]   \n",
       "3  [lmao, americans, in, the, comment, section, a...   \n",
       "4      [how, many, people, want, to, kill, him, now]   \n",
       "\n",
       "                                            com_remv  \\\n",
       "0  [japanese, trying, respectful, lo, gan, logan,...   \n",
       "1                                            [prick]   \n",
       "2                             [think, weeds, crying]   \n",
       "3  [lmao, americans, comment, section, acting, li...   \n",
       "4                         [many, people, want, kill]   \n",
       "\n",
       "                                           com_lemma  \\\n",
       "0  [japanese, trying, respectful, lo, gan, logan,...   \n",
       "1                                            [prick]   \n",
       "2                                 [think, weed, cry]   \n",
       "3  [lmao, american, comment, section, acting, lik...   \n",
       "4                         [many, people, want, kill]   \n",
       "\n",
       "                                            com_stem  \\\n",
       "0  [japanes, tri, respect, lo, gan, logan, care, ...   \n",
       "1                                            [prick]   \n",
       "2                                 [think, weed, cri]   \n",
       "3  [lmao, american, comment, section, act, like, ...   \n",
       "4                          [mani, peopl, want, kill]   \n",
       "\n",
       "                                            com_full  \\\n",
       "0          japanes tri respect lo gan logan care wtf   \n",
       "1                                              prick   \n",
       "2                                     think weed cri   \n",
       "3  lmao american comment section act like nuke ja...   \n",
       "4                               mani peopl want kill   \n",
       "\n",
       "                                          com_tagged  \\\n",
       "0  [[(h, NN), (o, NN), (w, NN)], [(c, VB), (a, DT...   \n",
       "1   [[(p, NN), (r, NN), (i, NN), (c, VBP), (k, NN)]]   \n",
       "2  [[(i, NN)], [(t, NN), (h, NN), (i, NN), (n, VB...   \n",
       "3  [[(l, NN), (m, VBZ), (a, DT), (o, NN)], [(a, D...   \n",
       "4  [[(h, NN), (o, NN), (w, NN)], [(m, VB), (a, DT...   \n",
       "\n",
       "                                        com_stem_str  \n",
       "0   japanes, tri, respect, lo, gan, logan, care, wtf  \n",
       "1                                              prick  \n",
       "2                                   think, weed, cri  \n",
       "3  lmao, american, comment, section, act, like, n...  \n",
       "4                            mani, peopl, want, kill  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = nlpFunction(df)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nlpFunction(df)\n",
    "data = nlpFunction(data)\n",
    "trump = nlpFunction(trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>com_token</th>\n",
       "      <th>com_remv</th>\n",
       "      <th>com_lemma</th>\n",
       "      <th>com_stem</th>\n",
       "      <th>com_full</th>\n",
       "      <th>com_tagged</th>\n",
       "      <th>com_stem_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can he do this  Japanese are trying to be ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[how, can, he, do, this, japanese, are, trying...</td>\n",
       "      <td>[japanese, trying, respectful, lo, gan, logan,...</td>\n",
       "      <td>[japanese, trying, respectful, lo, gan, logan,...</td>\n",
       "      <td>[japanes, tri, respect, lo, gan, logan, care, ...</td>\n",
       "      <td>japanes tri respect lo gan logan care wtf</td>\n",
       "      <td>[[(h, NN), (o, NN), (w, NN)], [(c, VB), (a, DT...</td>\n",
       "      <td>japanes, tri, respect, lo, gan, logan, care, wtf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prick</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[prick]</td>\n",
       "      <td>[prick]</td>\n",
       "      <td>[prick]</td>\n",
       "      <td>[prick]</td>\n",
       "      <td>prick</td>\n",
       "      <td>[[(p, NN), (r, NN), (i, NN), (c, VBP), (k, NN)]]</td>\n",
       "      <td>prick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I think all the weeds are crying</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, think, all, the, weeds, are, crying]</td>\n",
       "      <td>[think, weeds, crying]</td>\n",
       "      <td>[think, weed, cry]</td>\n",
       "      <td>[think, weed, cri]</td>\n",
       "      <td>think weed cri</td>\n",
       "      <td>[[(i, NN)], [(t, NN), (h, NN), (i, NN), (n, VB...</td>\n",
       "      <td>think, weed, cri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lmao Americans in the comment section are acti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[lmao, americans, in, the, comment, section, a...</td>\n",
       "      <td>[lmao, americans, comment, section, acting, li...</td>\n",
       "      <td>[lmao, american, comment, section, acting, lik...</td>\n",
       "      <td>[lmao, american, comment, section, act, like, ...</td>\n",
       "      <td>lmao american comment section act like nuke ja...</td>\n",
       "      <td>[[(l, NN), (m, VBZ), (a, DT), (o, NN)], [(a, D...</td>\n",
       "      <td>lmao, american, comment, section, act, like, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many people want to kill him now</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[how, many, people, want, to, kill, him, now]</td>\n",
       "      <td>[many, people, want, kill]</td>\n",
       "      <td>[many, people, want, kill]</td>\n",
       "      <td>[mani, peopl, want, kill]</td>\n",
       "      <td>mani peopl want kill</td>\n",
       "      <td>[[(h, NN), (o, NN), (w, NN)], [(m, VB), (a, DT...</td>\n",
       "      <td>mani, peopl, want, kill</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  label  \\\n",
       "0  How can he do this  Japanese are trying to be ...    NaN   \n",
       "1                                             Prick     NaN   \n",
       "2                  I think all the weeds are crying     NaN   \n",
       "3  Lmao Americans in the comment section are acti...    NaN   \n",
       "4             How many people want to kill him now      NaN   \n",
       "\n",
       "                                           com_token  \\\n",
       "0  [how, can, he, do, this, japanese, are, trying...   \n",
       "1                                            [prick]   \n",
       "2           [i, think, all, the, weeds, are, crying]   \n",
       "3  [lmao, americans, in, the, comment, section, a...   \n",
       "4      [how, many, people, want, to, kill, him, now]   \n",
       "\n",
       "                                            com_remv  \\\n",
       "0  [japanese, trying, respectful, lo, gan, logan,...   \n",
       "1                                            [prick]   \n",
       "2                             [think, weeds, crying]   \n",
       "3  [lmao, americans, comment, section, acting, li...   \n",
       "4                         [many, people, want, kill]   \n",
       "\n",
       "                                           com_lemma  \\\n",
       "0  [japanese, trying, respectful, lo, gan, logan,...   \n",
       "1                                            [prick]   \n",
       "2                                 [think, weed, cry]   \n",
       "3  [lmao, american, comment, section, acting, lik...   \n",
       "4                         [many, people, want, kill]   \n",
       "\n",
       "                                            com_stem  \\\n",
       "0  [japanes, tri, respect, lo, gan, logan, care, ...   \n",
       "1                                            [prick]   \n",
       "2                                 [think, weed, cri]   \n",
       "3  [lmao, american, comment, section, act, like, ...   \n",
       "4                          [mani, peopl, want, kill]   \n",
       "\n",
       "                                            com_full  \\\n",
       "0          japanes tri respect lo gan logan care wtf   \n",
       "1                                              prick   \n",
       "2                                     think weed cri   \n",
       "3  lmao american comment section act like nuke ja...   \n",
       "4                               mani peopl want kill   \n",
       "\n",
       "                                          com_tagged  \\\n",
       "0  [[(h, NN), (o, NN), (w, NN)], [(c, VB), (a, DT...   \n",
       "1   [[(p, NN), (r, NN), (i, NN), (c, VBP), (k, NN)]]   \n",
       "2  [[(i, NN)], [(t, NN), (h, NN), (i, NN), (n, VB...   \n",
       "3  [[(l, NN), (m, VBZ), (a, DT), (o, NN)], [(a, D...   \n",
       "4  [[(h, NN), (o, NN), (w, NN)], [(m, VB), (a, DT...   \n",
       "\n",
       "                                        com_stem_str  \n",
       "0   japanes, tri, respect, lo, gan, logan, care, wtf  \n",
       "1                                              prick  \n",
       "2                                   think, weed, cri  \n",
       "3  lmao, american, comment, section, act, like, n...  \n",
       "4                            mani, peopl, want, kill  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-778a535722e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mevaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram_word_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    " \n",
    "def bigram_word_feats(DF, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(DF[\"\"])\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])\n",
    " \n",
    "evaluate_classifier(bigram_word_feats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Split into Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn # machine learning\n",
    "from sklearn.model_selection import train_test_split # splitting up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[\"com_stem_str\"]\n",
    "X_test = trump[\"com_stem_str\"]\n",
    "Y_train = data[\"label\"]\n",
    "Y_test = trump[\"label\"]\n",
    "X_user = df[\"com_stem_str\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lengths training variables: ', len(X_train),\",\", len(Y_train))\n",
    "print('lengths testing variables: ', len(X_test),\",\", len(Y_test), '\\n')\n",
    "\n",
    "print('Are there any missing values?', \n",
    "      '\\n * Training:', pd.isnull(X_train).values.any(), ',', pd.isnull(Y_train).values.any(), \n",
    "      '\\n * Testing: ', pd.isnull(X_test).values.any(), \",\", pd.isnull(Y_test).values.any())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Transform Data to Counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "xtrain = tfidf.fit_transform(X_train) # transform and fit training data\n",
    "xtest = tfidf.transform(X_test) # transform trump test data from fitted transformer\n",
    "xuser = tfidf.transform(X_user) # transform user selected comments to predict on\n",
    "\n",
    "data_trans= tfidf.transform(data[\"com_stem_str\"]) # same as X_train...transform entire dataset for cross validation\n",
    "df_trans = tfidf.transform(df[\"com_stem_str\"]) # same as X_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm # support vector machine\n",
    "from sklearn import metrics # for accuracy/ precision\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier # Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Multinomial Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(xtrain, Y_train) # fit the model on the training data word counts and training data lables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Predictions:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_predict = mnb.predict(xtest) # make our y predictions (labels) on the comment test data\n",
    "mnb_acc = metrics.accuracy_score(Y_test, mnb_predict)\n",
    "print('We obtained ', round(mnb_acc, 6), '% accuracy for the model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(Y_test, mnb_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(Y_test, mnb_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation of Accuracy:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(mnb, xtest, Y_test, cv=5) # 5 fold cross validation\n",
    "print(\"Confidence Interval for Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='sag', max_iter=100, random_state=42, multi_class=\"multinomial\") # set multinomial setting for multiclass data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(xtrain, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predict = lr.predict(xtest)\n",
    "lr_acc = metrics.accuracy_score(Y_test, lr_predict)\n",
    "print('We obtained ', round(lr_acc, 6), '% accuracy for the logistic regression model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Report:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(Y_test, lr_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(Y_test, lr_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(lr, xtest, Y_test, cv=5) # 5 fold cross validation\n",
    "print(\"Confidence Interval for Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the Model & Predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = svm.SVC()\n",
    "svm.fit(xtrain, Y_train)\n",
    "svm_predict = svm.predict(xtest)\n",
    "svm_acc = metrics.accuracy_score(Y_test, svm_predict)\n",
    "print('We obtained ', round(svm_acc, 6), '% accuracy for the SVM model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Report:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(Y_test, mnb_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(Y_test, lr_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(svm, xtest, Y_test, cv=5) # 5 fold cross validation\n",
    "print(\"Confidence Interval for Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting Model & Predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier # k-NN ensemble method\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(xtrain, Y_train)\n",
    "\n",
    "knn_predict = knn.predict(xtest)\n",
    "knn_acc = metrics.accuracy_score(Y_test, knn_predict)\n",
    "print('We obtained ', round(knn_acc, 6), '% accuracy for the KNN Bagging model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(knn, xtest, Y_test, cv=5) # 5 fold cross validation\n",
    "print(\"Confidence Interval for Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting Model & Predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier # random forest ensemble method\n",
    "\n",
    "ranfor = RandomForestClassifier(n_estimators=10, random_state=10)\n",
    "ranfor = ranfor.fit(xtrain, Y_train)\n",
    "\n",
    "rf_predict = ranfor.predict(xtest)\n",
    "rf_acc = metrics.accuracy_score(Y_test, rf_predict)\n",
    "print('We obtained ', round(rf_acc, 6), '% accuracy for the Random Forest model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(ranfor, xtest, Y_test, cv=5) # 5 fold cross validation\n",
    "print(\"Confidence Interval for Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(xtrain, Y_train)\n",
    "xgb_pred = xgb.predict(xtest)\n",
    "xgb_acc = metrics.accuracy_score(Y_test, xgb_pred)\n",
    "print('We obtained ', round(xgb_acc, 6), '% accuracy for the XGB Bagging model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(xgb, xtest, Y_test, cv=5) # 5 fold cross validation\n",
    "print(\"Confidence Interval for Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Data Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Table of Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTable = pd.DataFrame(columns=['Naive Bayes','Support Vect Machine','Logistic Regression', 'K-NN', 'Random Forest'],\n",
    "                   index=[\"Accuracy\"])\n",
    "myTable['Naive Bayes']=mnb_acc; myTable['Support Vect Machine']=svm_acc; myTable['Logistic Regression']=lr_acc\n",
    "myTable['K-NN']= knn_acc; myTable['Random Forest']= rf_acc\n",
    "myTable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
